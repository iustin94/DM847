unlabelled <- get_peaks("*_out2.csv")
unlabelled.data <- get_peaks_data(unlabelled)
unlabelled.n_clust <- peaks.n_clust
unlabelled.kmeans <- kmeans(unlabelled.data, unlabelled.n_clust)
unlabelled.matrix <- build_matrix(unlabelled, unlabelled.kmeans$cluster, unlabelled.n_clust)
ulabelled.the_matrix <- build_training_matrix(unlabelled.matrix)
the_result <- predict_result(tree.model2, ulabelled.the_matrix)
View(BD18_1711291646)
View(BD18_1711291702_out)
View(training)
View(peaks.data)
View(peaks.kmeans)
View(peaks.data)
View(peaks.matrix)
View(peaks.kmeans)
View(peaks)
View(peaks.data)
peaks.data <- get_peaks_data(peaks)
peaks.n_clust <- get_nclust(peaks.data)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 100)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 100)
plot(x = peaks.data[,2], y = peaks.data[,3], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = "K-means on peak data")
# Build: matrix, training matrix
peaks.labels <- peaks_labels
peaks.matrix <- build_matrix(peaks, peaks.kmeans$cluster, peaks.n_clust)
View(p_data)
View(matrixData)
View(peaks)
View(peaks.data)
peaks <- get_peaks("*_out.csv")
View(peaks)
set.seed(42)
DEBUG = TRUE
project_path <- getwd()
load_raw_data("data")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data")
}
peaks <- get_peaks("*_out.csv")
View(peaks)
peaks.data <- get_peaks_data(peaks)
View(peaks.data)
peaks.n_clust <- get_nclust(peaks.data)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 100)
plot(x = peaks.data[,2], y = peaks.data[,3], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = "K-means on peak data")
# Build: matrix, training matrix
peaks.labels <- peaks_labels
peaks.matrix <- build_matrix(peaks, peaks.kmeans$cluster, peaks.n_clust)
View(peaks.matrix)
View(peaks.matrix)
setwd("~/Desktop/DM847")
set.seed(42)
DEBUG = TRUE
project_path <- getwd()
load_raw_data("data")
source("helper_functions.R") # Import helper functions
load_raw_data("data")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data")
}
peaks <- get_peaks("*_out.csv")
peaks.data <- get_peaks_data(peaks)
peaks.n_clust <- get_nclust(peaks.data)
View(peaks.data)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 100)
plot(x = peaks.data[,2], y = peaks.data[,3], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = "K-means on peak data")
plot(x = peaks.data[,1], y = peaks.data[,2], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = "K-means on peak data")
# Build: matrix, training matrix
peaks.labels <- peaks_labels
View(peaks)
View(peaks.data)
peaks.matrix <- build_matrix(peaks, peaks.kmeans$cluster, peaks.n_clust)
View(peaks.matrix)
training <- build_training_matrix(peaks.matrix, peaks_labels)
# SPLIT DATA INTO TRAINING AND TEST DATA (75%)
train_split <- floor(0.75 * nrow(training))
train_ind <- sample(seq_len(nrow(training)), size = train_split)
rf.train <- training[train_ind, ]
rf.test <- training[-train_ind, ]
rf.model <- randomForest(candy ~ ., data = rf.train, ntree = 500)
rf.model
predict(rf.model, rf.test)
# getTree(rf.model, k = 81)
# plot(rf.model)
reprtree:::plot.getTree(rf.model)
# 5-FOLD CROSS-VALIDATION
rf.num <- 5
fold.trainControl <- trainControl(method = "cv", number = rf.num, classProbs = TRUE, savePredictions = TRUE, allowParallel = TRUE, p = 0.75)
fold.model <- train(candy ~ ., data = training, trControl = fold.trainControl, method = "rf", tuneLength = 10, metric = "Accuracy")
fold.model
plot(fold.model)
# REPORT MEAN, ACCURACY, SENSITIVITY AND SPECIFICITY
predicted <- predict(fold.model, rf.test)
sensitivity(rf.test$candy, predicted)
specificity(rf.test$candy, predicted)
accuracy(rf.test$candy, predicted)
# 5 PEAKS USING Gini index
gini.peaks <- build_gini_index(fold.model)
# FORMULA
tree.formula <- as.formula(
paste("candy ~ ",
paste(gini.peaks$peaks, collapse = "+"),
sep = "")
)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
#########################################################################################
# UNLABELED DATA
load_raw_data("unlabelled_candy_raw")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out2.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data", "_out2.csv")
}
unlabelled <- get_peaks("*_out2.csv")
unlabelled.data <- get_peaks_data(unlabelled)
unlabelled.n_clust <- peaks.n_clust
unlabelled.kmeans <- kmeans(unlabelled.data, unlabelled.n_clust)
unlabelled.matrix <- build_matrix(unlabelled, unlabelled.kmeans$cluster, unlabelled.n_clust)
ulabelled.the_matrix <- build_training_matrix(unlabelled.matrix)
the_result <- predict_result(tree.model2, ulabelled.the_matrix)
predict(tree.model1, ulabelled.the_matrix)
predict(tree.model2, ulabelled.the_matrix)
# APPROACH #1
bestmtry <- tuneRF(rf.train, rf.test, stepFactor=1.5, improve=0.05, ntree=500)
# APPROACH #1
bestmtry <- tuneRF(rf.train$candy, rf.test$candy, stepFactor=1.5, improve=0.05, ntree=500)
# APPROACH #1
bestmtry <- tuneRF(rf.train, rf.test, stepFactor=1.5, improve=0.05, ntree=500)
iris
iris[, 1:3]
View(training)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.05, ntree=500)
bestmtry
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.005, ntree=500)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.0005, ntree=500)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.0005, ntree=200)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.5, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=300)
# APPROACH #1
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
# APPROACH #1
plot(rf.model)
rf.model$votes
rf.model$forest
rf.model
rf.model$err.rate
length(rf.model$err.rate)
rf.model$confusion
rf.model$importance
# APPROACH #1
plot(rf.model)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500, mtry = 5)
reprtree:::plot.getTree(tree.model1)
predict(tree.model1, ulabelled.the_matrix)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400, mtry = 5)
reprtree:::plot.getTree(tree.model1)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400, mtry = 17)
bestmtry <- tuneRF(training[,1:peaks.n_clust], training[,peaks.n_clust + 1], stepFactor=1.95, improve=0.0005, ntree=400)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400, mtry = 9)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 300)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 300)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 300)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 300)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 300)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 400)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500)
reprtree:::plot.getTree(tree.model1)
predict(tree.model1, ulabelled.the_matrix)
source("helper_functions.R") # Import helper functions
set.seed(42)
DEBUG = TRUE
project_path <- getwd()
load_raw_data("data")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data")
}
peaks <- get_peaks("*_out.csv")
peaks.data <- get_peaks_data(peaks)
peaks.n_clust <- get_nclust(peaks.data)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 1000)
plot(x = peaks.data[,1], y = peaks.data[,2], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = "K-means on peak data")
# Build: matrix, training matrix
peaks.labels <- peaks_labels
peaks.matrix <- build_matrix(peaks, peaks.kmeans$cluster, peaks.n_clust)
training <- build_training_matrix(peaks.matrix, peaks_labels)
# SPLIT DATA INTO TRAINING AND TEST DATA (75%)
train_split <- floor(0.75 * nrow(training))
train_ind <- sample(seq_len(nrow(training)), size = train_split)
rf.train <- training[train_ind, ]
rf.test <- training[-train_ind, ]
rf.model <- randomForest(candy ~ ., data = rf.train, ntree = 500)
rf.model
predict(rf.model, rf.test)
# getTree(rf.model, k = 81)
# plot(rf.model)
reprtree:::plot.getTree(rf.model)
# 5-FOLD CROSS-VALIDATION
rf.num <- 5
fold.trainControl <- trainControl(method = "cv", number = rf.num, classProbs = TRUE, savePredictions = TRUE, allowParallel = TRUE, p = 0.75)
fold.model <- train(candy ~ ., data = training, trControl = fold.trainControl, method = "rf", tuneLength = 10, metric = "Accuracy")
fold.model
plot(fold.model)
# REPORT MEAN, ACCURACY, SENSITIVITY AND SPECIFICITY
predicted <- predict(fold.model, rf.test)
sensitivity(rf.test$candy, predicted)
specificity(rf.test$candy, predicted)
accuracy(rf.test$candy, predicted)
# 5 PEAKS USING Gini index
gini.peaks <- build_gini_index(fold.model)
# FORMULA
tree.formula <- as.formula(
paste("candy ~ ",
paste(gini.peaks$peaks, collapse = "+"),
sep = "")
)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500, mtry = 28)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training, ntree = 500, mtry = (1:28))
reprtree:::plot.getTree(tree.model1)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
#########################################################################################
# UNLABELED DATA
load_raw_data("unlabelled_candy_raw")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out2.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data", "_out2.csv")
}
unlabelled <- get_peaks("*_out2.csv")
unlabelled.data <- get_peaks_data(unlabelled)
unlabelled.n_clust <- peaks.n_clust
unlabelled.kmeans <- kmeans(unlabelled.data, unlabelled.n_clust)
unlabelled.matrix <- build_matrix(unlabelled, unlabelled.kmeans$cluster, unlabelled.n_clust)
ulabelled.the_matrix <- build_training_matrix(unlabelled.matrix)
predict(tree.model1, ulabelled.the_matrix)
predict(tree.model2, ulabelled.the_matrix)
# FORMULA
tree.formula <- as.formula(
paste("candy ~ ",
paste(gini.peaks$peaks, collapse = "+"),
sep = "")
)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training)
reprtree:::plot.getTree(tree.model1)
# APPROACH #1
tree.model1 <- randomForest(tree.formula, data = training)
reprtree:::plot.getTree(tree.model1)
tree.model1
tree.model1$err.rate
tree.model1$oob.times
tree.model1
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 30, cp = 0.001),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 30, cp = 0.001),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0.001),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0.001),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0.001),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
predict(tree.model2, ulabelled.the_matrix)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0.01),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
View(peaks.matrix)
View(peaks.data)
View(peaks.matrix)
View(peaks.labels)
source("helper_functions.R") # Import helper functions
set.seed(1437)
DEBUG = TRUE
project_path <- getwd()
load_raw_data("data")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data")
}
peaks <- get_peaks("*_out.csv")
peaks.data <- get_peaks_data(peaks)
peaks.n_clust <- get_nclust(peaks.data)
peaks.kmeans <- kmeans(peaks.data, peaks.n_clust, nstart = 1000)
plot(x = peaks.data[,1], y = peaks.data[,2], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = paste(peaks.kmeans, "k-means clusters", collapse = " "))
plot(x = peaks.data[,1], y = peaks.data[,2], col=peaks.kmeans$cluster, pch=20,
xlab = "t", ylab = "r", main = paste(peaks.n_clust, "k-means clusters", collapse = " "))
# Build: matrix, training matrix
peaks.labels <- peaks_labels
peaks.matrix <- build_matrix(peaks, peaks.kmeans$cluster, peaks.n_clust)
training <- build_training_matrix(peaks.matrix, peaks_labels)
# SPLIT DATA INTO TRAINING AND TEST DATA (75%)
train_split <- floor(0.75 * nrow(training))
train_ind <- sample(seq_len(nrow(training)), size = train_split)
rf.train <- training[train_ind, ]
rf.test <- training[-train_ind, ]
rf.model <- randomForest(candy ~ ., data = rf.train, ntree = 500)
rf.model
predict(rf.model, rf.test)
# 5-FOLD CROSS-VALIDATION
rf.num <- 5
fold.trainControl <- trainControl(method = "cv", number = rf.num, classProbs = TRUE, savePredictions = TRUE, allowParallel = TRUE, p = 0.75)
fold.model <- train(candy ~ ., data = training, trControl = fold.trainControl, method = "rf", tuneLength = 20, metric = "Accuracy")
fold.model
plot(fold.model)
# REPORT MEAN, ACCURACY, SENSITIVITY AND SPECIFICITY
predicted <- predict(fold.model, rf.test)
sensitivity(rf.test$candy, predicted)
specificity(rf.test$candy, predicted)
accuracy(rf.test$candy, predicted)
# 5 PEAKS USING Gini index
gini.peaks <- build_gini_index(fold.model)
# FORMULA
tree.formula <- as.formula(
paste("candy ~ ",
paste(gini.peaks$peaks, collapse = "+"),
sep = "")
)
# APPROACH #2
tree.model2 <- rpart(tree.formula,
data=training,
method="class",
control=rpart.control(minsplit = 2, cp = 0.01),
model = TRUE
)
summary(tree.model2)
rpart.plot(tree.model2, type = 4)
plot_dt()
#########################################################################################
# UNLABELED DATA
load_raw_data("unlabelled_candy_raw")
if (DEBUG == TRUE) {
setwd(paste(project_path, "corrected_data", sep = "/"))
temp = list.files(pattern="*_out2.csv")
list2env(
lapply(setNames(temp, make.names(gsub("*.csv$", "", temp))),
read.table, header=TRUE), envir = .GlobalEnv
)
} else {
build_peaks("data", "peax", "corrected_data", "_out2.csv")
}
unlabelled <- get_peaks("*_out2.csv")
unlabelled.data <- get_peaks_data(unlabelled)
unlabelled.n_clust <- peaks.n_clust
unlabelled.kmeans <- kmeans(unlabelled.data, unlabelled.n_clust)
unlabelled.matrix <- build_matrix(unlabelled, unlabelled.kmeans$cluster, unlabelled.n_clust)
ulabelled.the_matrix <- build_training_matrix(unlabelled.matrix)
# predict(tree.model1, ulabelled.the_matrix)
predict(tree.model2, ulabelled.the_matrix)
the_result <- predict_result(tree.model2, ulabelled.the_matrix)
